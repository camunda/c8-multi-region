---
name: reusable_teleport_operational_procedure

on:
    workflow_call:
        inputs:
            helmChartVersion:
                type: string
                description: Version of the Helm chart to deploy.
                required: true

            helmChartName:
                description: Optional Helm chart name
                type: string

            globalImageTag:
                description: Optional global image tag
                type: string

            awsProfile:
                description: AWS profile to use
                type: string
                default: infex

            testsTfBinaryName:
                description: Terraform binary name for tests
                type: string
                default: terraform

            namespacePrefix:
                description: Prefix for namespaces
                type: string
                default: infraex-

            clusterName:
                description: EKS cluster name
                type: string
                default: camunda-ci-eks

            labels:
                description: Labels to apply
                type: string
                default: camunda.cloud/ephemeral=true
            annotations:
                description: 'Annotations to apply (format: key1=val1 key2=val2)'
                required: false
                type: string
                default: janitor/ttl=2h

            cluster:
                description: Cluster identifier (same for Cluster 0 & 1)
                type: string
                default: camunda.teleport.sh-camunda-ci-eks

            backupBucket:
                description: Backup bucket name
                type: string
                default: tests-c8-multi-region-es-eu-central-1

            helmReleaseName:
                description: Helm release name
                type: string
                default: camunda

            zeebeClusterSize:
                description: Zeebe cluster size
                type: number
                default: 8

            deployOnly:
                description: If true, only deploy Camunda and skip failover/failback tests
                type: boolean
                default: false

            migrationEnabled:
                description: If true, enable migration mode
                type: boolean
                default: false
            cluster_0_namespace:
                description: Allows setting a custom namespace for cluster 0
                type: string
            cluster_1_namespace:
                description: Allows setting a custom namespace for cluster 1
                type: string
            backup_name:
                description: Allows setting a custom backup name
                type: string
            overwriteBranch:
                description: Optional branch to checkout instead of inferring from helm chart version
                type: string
            keepAlive:
                description: Whether to keep the clusters alive after the procedure
                type: boolean
                default: false
            skipFailover:
                description: Whether to skip the tests after deployment
                type: boolean
                default: false
            skipFailback:
                description: Whether to skip the tests after deployment
                type: boolean
                default: false
        outputs:
            CLUSTER_0_NAMESPACE:
                description: Namespace for cluster 0
                value: ${{ jobs.teleport-setup.outputs.CLUSTER_0_NAMESPACE }}
            CLUSTER_1_NAMESPACE:
                description: Namespace for cluster 1
                value: ${{ jobs.teleport-setup.outputs.CLUSTER_1_NAMESPACE }}
            BACKUP_NAME:
                description: Random ID used for backup name
                value: ${{ jobs.teleport-setup.outputs.BACKUP_NAME }}

        secrets:
            VAULT_ADDR:
                required: true
            VAULT_ROLE_ID:
                required: true
            VAULT_SECRET_ID:
                required: true

permissions:
    contents: read
    id-token: write

env:
    AWS_PROFILE: ${{ inputs.awsProfile }}
    TESTS_TF_BINARY_NAME: ${{ inputs.testsTfBinaryName }}
    NAMESPACE_PREFIX: ${{ inputs.namespacePrefix }}
    CLUSTER_NAME: ${{ inputs.clusterName }}
    LABELS: ${{ inputs.labels }}
    ANNOTATIONS: ${{ inputs.annotations }}

    # Single input used for both cluster variables (to be able to reuse dual cluster tests)
    CLUSTER_0: ${{ inputs.cluster }}
    CLUSTER_1: ${{ inputs.cluster }}

    KUBECONFIG: ./kubeconfig

    BACKUP_BUCKET: ${{ inputs.backupBucket }}

    CAMUNDA_RELEASE_NAME: ${{ inputs.helmReleaseName }}

    ZEEBE_CLUSTER_SIZE: ${{ inputs.zeebeClusterSize }}
    HELM_CHART_VERSION: ${{ inputs.helmChartVersion }}

    CLUSTER_1_NAMESPACE: c8-multiregion-test-cluster-1
    CLUSTER_0_NAMESPACE: c8-multiregion-test-cluster-0

    TELEPORT: true

jobs:
    teleport-setup:
        runs-on: ubuntu-latest
        outputs:
            CLUSTER_0_NAMESPACE: ${{ steps.update-namespaces.outputs.CLUSTER_0_NAMESPACE }}
            CLUSTER_1_NAMESPACE: ${{ steps.update-namespaces.outputs.CLUSTER_1_NAMESPACE }}
            BACKUP_NAME: ${{ steps.update-namespaces.outputs.BACKUP_NAME }}

        steps:
            - name: Set optional environment variables conditionally
              run: |
                  if [ -n "${{ inputs.helmChartName }}" ]; then
                    echo "HELM_CHART_NAME=${{ inputs.helmChartName }}" >> "$GITHUB_ENV"
                  fi
                  if [ -n "${{ inputs.globalImageTag }}" ]; then
                    # With 8.7 and 8.8 being developed concurrently, the helm chart is 8.7 while the images are 8.8
                    # Therefore fallback atm on the helm chart defined image tags
                    # echo "GLOBAL_IMAGE_TAG=${{ inputs.globalImageTag }}" >> "$GITHUB_ENV"
                    echo "GLOBAL_IMAGE_TAG=${{ inputs.globalImageTag }}"
                  fi

            - name: Map Helm version to branch
              id: helm-version
              run: |
                  version=${{ inputs.helmChartVersion }}

                  if [[ $version == "SNAPSHOT" ]]; then
                    # we use the snapshot alpha chart of the Helm Chart
                    # We use the snapshot image of camunda/camunda
                    {
                        echo "HELM_CHART_VERSION=0.0.0-snapshot-alpha"
                        echo "HELM_CHART_NAME=oci://ghcr.io/camunda/helm/camunda-platform"
                    } >> "$GITHUB_ENV"
                    if [[ "$GITHUB_EVENT_NAME" == "pull_request" ]]; then
                      echo "branch=$GITHUB_HEAD_REF" >> "$GITHUB_ENV"
                    else
                      echo "branch=main" >> "$GITHUB_ENV"
                    fi
                  else
                    c8_version=$(curl -X 'GET' -s \
                    "https://artifacthub.io/api/v1/packages/helm/camunda/camunda-platform/${version}" \
                    -H "accept: application/json" | jq -r .app_version)
                    minor_version=$(echo "$c8_version" | cut -d '.' -f 2)
                    echo "branch=stable/8.${minor_version}" >> "$GITHUB_ENV"

                    # For PRs, overwrite to the head ref if the base ref matches the detected minor version
                    if [[ "$GITHUB_EVENT_NAME" == "pull_request" && "$GITHUB_BASE_REF" =~ 8.${minor_version} ]]; then
                      echo "Detected PR against matching stable branch, using head ref"
                      echo "branch=$GITHUB_HEAD_REF" >> "$GITHUB_ENV"
                    fi
                  fi

                  # For workflow_dispatch events, use the ref that triggered the workflow
                  if [[ "$GITHUB_EVENT_NAME" == "workflow_dispatch" ]]; then
                    echo "Detected workflow_dispatch event, using triggered ref - $GITHUB_REF_NAME"
                    echo "branch=$GITHUB_REF_NAME" >> "$GITHUB_ENV"
                  fi

                  if [ -n "${{ inputs.overwriteBranch }}" ]; then
                    echo "Overwriting branch to checkout with input: ${{ inputs.overwriteBranch }}"
                    echo "branch=${{ inputs.overwriteBranch }}" >> "$GITHUB_ENV"
                  fi

            - name: Checkout repository
              uses: actions/checkout@93cb6efe18208431cddfb8368fd83d5badbf9bfd # v5
              with:
                  ref: ${{ env.branch }}

            - name: Setup AWS and Tools
              uses: ./.github/actions/setup-aws
              with:
                  secrets: ${{ toJSON(secrets) }}

            - name: Install extra Go and use inbuilt caching
              uses: actions/setup-go@4dc6199c7b1a012772edbd06daecab0f50c9053c # v6
              with:
                  go-version-file: ./test/go.mod
                  cache-dependency-path: ./test/go.sum

            - name: Set up Teleport
              uses: teleport-actions/setup@a820ebbf1bc1a496efca348ad21042d6e8df73a6 # v1
              with:
                  # renovate: datasource=docker depName=public.ecr.aws/gravitational/teleport-ent-distroless
                  version: 18.2.10

            - name: Authenticate with Teleport
              uses: teleport-actions/auth-k8s@171cc0ad4b6b7cebcb33c672defe3f6dc58967ba # v2
              with:
                  proxy: camunda.teleport.sh:443
                  token: infra-ci-prod-github-action-infraex
                  kubernetes-cluster: camunda-ci-eks
                  certificate-ttl: 3h # Set a TTL for the certificate matching the job duration (worst case)

            - name: Write kubeconfig file
              id: write-kubeconfig
              run: |
                  kubectl config view --raw > ./test/kubeconfig

            - name: Update namespaces with prefix and random suffix
              id: update-namespaces
              run: |
                  set -euxo pipefail

                  RANDOM_ID="$(openssl rand -hex 3)"

                  BACKUP_NAME=${RANDOM_ID}
                  CLUSTER_1_NAMESPACE="${NAMESPACE_PREFIX}${CLUSTER_1_NAMESPACE}-${RANDOM_ID}"
                  CLUSTER_0_NAMESPACE="${NAMESPACE_PREFIX}${CLUSTER_0_NAMESPACE}-${RANDOM_ID}"

                  # Override with inputs if provided
                  if [ -n "${{ inputs.cluster_0_namespace }}" ]; then
                    CLUSTER_0_NAMESPACE="${{ inputs.cluster_0_namespace }}"
                  fi

                  if [ -n "${{ inputs.cluster_1_namespace }}" ]; then
                    CLUSTER_1_NAMESPACE="${{ inputs.cluster_1_namespace }}"
                  fi

                  if [ -n "${{ inputs.backup_name }}" ]; then
                    RANDOM_ID="${{ inputs.backup_name }}"
                  fi

                  # Write the updated values to the GitHub Actions environment for subsequent steps.
                  {
                    echo "BACKUP_NAME=${BACKUP_NAME}"
                    echo "CLUSTER_1_NAMESPACE=${CLUSTER_1_NAMESPACE}"
                    echo "CLUSTER_0_NAMESPACE=${CLUSTER_0_NAMESPACE}"
                    echo "CAMUNDA_NAMESPACE_0=${CLUSTER_0_NAMESPACE}"
                    echo "CAMUNDA_NAMESPACE_1=${CLUSTER_1_NAMESPACE}"
                  } | tee -a "$GITHUB_ENV" "$GITHUB_OUTPUT"


            - name: Import Secrets
              if: inputs.migrationEnabled == false
              id: secrets
              uses: hashicorp/vault-action@4c06c5ccf5c0761b6029f56cfb1dcf5565918a3b # v3
              with:
                  url: ${{ secrets.VAULT_ADDR }}
                  method: approle
                  roleId: ${{ secrets.VAULT_ROLE_ID }}
                  secretId: ${{ secrets.VAULT_SECRET_ID }}
                  secrets: |
                      secret/data/products/infrastructure-experience/ci/common AWS_ACCESS_KEY | S3_BACKUP_ACCESS_KEY;
                      secret/data/products/infrastructure-experience/ci/common AWS_SECRET_KEY | S3_BACKUP_SECRET_KEY;

            - name: Create namespaces and secrets
              if: inputs.migrationEnabled == false
              id: create-namespaces
              env:
                  AWS_SECRET_ACCESS_KEY_ES: ${{ steps.secrets.outputs.S3_BACKUP_SECRET_KEY }}
                  AWS_ACCESS_KEY_ES: ${{ steps.secrets.outputs.S3_BACKUP_ACCESS_KEY }}
              working-directory: ./test
              run: |
                  set -euxo pipefail
                  go test --count=1 -v -timeout 9m -run TestClusterPrerequisites

            - name: Label and annotate namespaces and secrets
              if: inputs.migrationEnabled == false
              run: |
                  set -euxo pipefail

                  apply_labels_and_annotations() {
                    local namespace="$1"

                    # Apply labels to namespace
                    if [ -n "${LABELS:-}" ]; then
                      kubectl label namespace "$namespace" "${LABELS}" --overwrite
                    fi

                    # Apply annotations to namespace
                    if [ -n "${ANNOTATIONS:-}" ]; then
                      for ann in ${ANNOTATIONS}; do
                        kubectl annotate namespace "$namespace" "$ann" --overwrite
                      done
                    fi

                    # Apply labels and annotations to all secrets in the namespace
                    local secrets
                    secrets="$(kubectl get secrets -n "$namespace" -o jsonpath='{.items[*].metadata.name}')"
                    for secret in $secrets; do
                      if [ -n "${LABELS:-}" ]; then
                        kubectl label secret "$secret" -n "$namespace" "${LABELS}" --overwrite
                      fi
                      if [ -n "${ANNOTATIONS:-}" ]; then
                        for ann in ${ANNOTATIONS}; do
                          kubectl annotate secret "$secret" -n "$namespace" "$ann" --overwrite
                        done
                      fi
                    done
                  }

                  # Combine cluster-0 and cluster-1 arrays into one comma-separated list
                  IFS=',' read -r -a ALL_NAMESPACES <<< "${CLUSTER_0_NAMESPACE},${CLUSTER_1_NAMESPACE}"
                  for namespace in "${ALL_NAMESPACES[@]}"; do
                    apply_labels_and_annotations "$namespace"
                  done

            - name: Deploy dual region Camunda - ${{ inputs.helmChartVersion }}
              if: inputs.migrationEnabled == false
              id: deploy-camunda
              working-directory: ./test
              timeout-minutes: 31
              run: |
                  set -euxo pipefail
                  go test --count=1 -v -timeout 30m -run TestAWSDeployDualRegCamunda

            - name: Deploy dual region Camunda with migration - ${{ inputs.helmChartVersion }}
              if: inputs.migrationEnabled == true
              id: deploy-camunda-migration
              working-directory: ./test
              timeout-minutes: 46
              run: |
                  set -euxo pipefail
                  go test --count=1 -v -timeout 45m -run TestMigrationDualReg

                  # 8.7 process instance created during deploy offset
                  echo "MIGRATION_OFFSET=1" >> "$GITHUB_ENV"

            - name: Collect migration logs
              if: always() && inputs.migrationEnabled == true
              continue-on-error: true
              run: |
                  echo "===== Collecting migration job logs ====="
                  mkdir -p ./test/migration_logs

                  # Get migration job status
                  echo "--- Migration job status in cluster 0 ---"
                  kubectl get jobs -l app.kubernetes.io/component=orchestration-migration -n ${{ env.CLUSTER_0_NAMESPACE }} || true
                  echo "--- Migration job status in cluster 1 ---"
                  kubectl get jobs -l app.kubernetes.io/component=orchestration-migration -n ${{ env.CLUSTER_1_NAMESPACE }} || true

                  # Get migration pods status
                  echo "--- Migration pods in cluster 0 ---"
                  kubectl get pods -l app.kubernetes.io/component=orchestration-migration -n ${{ env.CLUSTER_0_NAMESPACE }} || true
                  echo "--- Migration pods in cluster 1 ---"
                  kubectl get pods -l app.kubernetes.io/component=orchestration-migration -n ${{ env.CLUSTER_1_NAMESPACE }} || true

                  # Collect migration job logs from cluster 0
                  kubectl logs -l app.kubernetes.io/component=orchestration-migration \
                    --tail=-1 \
                    --timestamps=true \
                    --prefix=true \
                    --all-containers=true \
                    -n ${{ env.CLUSTER_0_NAMESPACE }} > "./test/migration_logs/migration_${{ env.CLUSTER_0_NAMESPACE }}_logs.txt" || echo "No migration logs in cluster 0"

                  # Collect migration job logs from cluster 1
                  kubectl logs -l app.kubernetes.io/component=orchestration-migration \
                    --tail=-1 \
                    --timestamps=true \
                    --prefix=true \
                    --all-containers=true \
                    -n ${{ env.CLUSTER_1_NAMESPACE }} > "./test/migration_logs/migration_${{ env.CLUSTER_1_NAMESPACE }}_logs.txt" || echo "No migration logs in cluster 1"

                  # Collect importer logs from cluster 0
                  kubectl logs -l app.kubernetes.io/component=orchestration-importer \
                    --tail=-1 \
                    --timestamps=true \
                    --prefix=true \
                    --all-containers=true \
                    -n ${{ env.CLUSTER_0_NAMESPACE }} > "./test/migration_logs/importer_${{ env.CLUSTER_0_NAMESPACE }}_logs.txt" || echo "No importer logs in cluster 0"

                  # Collect importer logs from cluster 1
                  kubectl logs -l app.kubernetes.io/component=orchestration-importer \
                    --tail=-1 \
                    --timestamps=true \
                    --prefix=true \
                    --all-containers=true \
                    -n ${{ env.CLUSTER_1_NAMESPACE }} > "./test/migration_logs/importer_${{ env.CLUSTER_1_NAMESPACE }}_logs.txt" || echo "No importer logs in cluster 1"

                  # Describe migration jobs for debugging
                  kubectl describe jobs -l app.kubernetes.io/component=orchestration-migration \
                    -n ${{ env.CLUSTER_0_NAMESPACE }} > "./test/migration_logs/migration_job_describe_cluster0.txt" || true
                  kubectl describe jobs -l app.kubernetes.io/component=orchestration-migration \
                    -n ${{ env.CLUSTER_1_NAMESPACE }} > "./test/migration_logs/migration_job_describe_cluster1.txt" || true

                  echo "===== Migration logs collected ====="
                  ls -la ./test/migration_logs/ || true

            - name: Show pending pod count in each namespace after initial deployment
              id: pending-check-1
              if: always()
              run: |
                  echo "===== Checking pending pods in CLUSTER_0_NAMESPACE after initial deployment ====="
                  # List all pods that are in Pending status, then count them.
                  kubectl get pods -n "${CLUSTER_0_NAMESPACE}" --field-selector=status.phase=Pending
                  num_pending_0=$(kubectl get pods -n "${CLUSTER_0_NAMESPACE}" --field-selector=status.phase=Pending --no-headers | wc -l)
                  echo "Number of pending pods in ${CLUSTER_0_NAMESPACE}: ${num_pending_0}"

                  echo "===== Checking pending pods in CLUSTER_1_NAMESPACE after initial deployment ====="
                  kubectl get pods -n "${CLUSTER_1_NAMESPACE}" --field-selector=status.phase=Pending
                  num_pending_1=$(kubectl get pods -n "${CLUSTER_1_NAMESPACE}" --field-selector=status.phase=Pending --no-headers | wc -l)
                  echo "Number of pending pods in ${CLUSTER_1_NAMESPACE}: ${num_pending_1}"

                  total_pending=$((num_pending_0 + num_pending_1))
                  if [ "$total_pending" -gt 0 ]; then
                   echo "::error ::PendingPodsError: Found $total_pending pending pods across namespaces."
                  fi

            - name: Failover New - ${{ inputs.helmChartVersion }}
              if: inputs.deployOnly == false && inputs.skipFailover == false
              working-directory: ./test
              timeout-minutes: 21
              run: |
                  set -euxo pipefail
                  go test --count=1 -v -timeout 20m -run TestAWSDualRegFailover_8_6_plus

            - name: Set start timestamp for Failback
              if: inputs.deployOnly == false && inputs.skipFailback == false
              id: failback-start
              run: |
                  set -euxo pipefail
                  printf 'timestamp=%(%s)T\n' >> "$GITHUB_OUTPUT"
            - name: Failback New - ${{ inputs.helmChartVersion }}
              if: inputs.deployOnly == false && inputs.skipFailback == false
              working-directory: ./test
              timeout-minutes: 46
              run: |
                  set -euxo pipefail
                  go test --count=1 -v -timeout 45m -run TestAWSDualRegFailback_8_6_plus

            - name: Show pending pod count in each namespace after failback-start
              id: pending-check-2
              if: always() && inputs.deployOnly == false && inputs.skipFailback == false
              run: |
                  echo "===== Checking pending pods in CLUSTER_0_NAMESPACE after Failback ====="
                  # List all pods that are in Pending status, then count them.
                  kubectl get pods -n "${CLUSTER_0_NAMESPACE}" --field-selector=status.phase=Pending
                  num_pending_0=$(kubectl get pods -n "${CLUSTER_0_NAMESPACE}" --field-selector=status.phase=Pending --no-headers | wc -l)
                  echo "Number of pending pods in ${CLUSTER_0_NAMESPACE}: ${num_pending_0}"

                  echo "===== Checking pending pods in CLUSTER_1_NAMESPACE after Failback ====="
                  kubectl get pods -n "${CLUSTER_1_NAMESPACE}" --field-selector=status.phase=Pending
                  num_pending_1=$(kubectl get pods -n "${CLUSTER_1_NAMESPACE}" --field-selector=status.phase=Pending --no-headers | wc -l)
                  echo "Number of pending pods in ${CLUSTER_1_NAMESPACE}: ${num_pending_1}"

                  total_pending=$((num_pending_0 + num_pending_1))
                  if [ "$total_pending" -gt 0 ]; then
                   echo "::error ::PendingPodsError: Found $total_pending pending pods across namespaces."
                  fi

            - name: Calculate Failback duration
              if: inputs.deployOnly == false && inputs.skipFailback == false
              run: |
                  set -euxo pipefail
                  printf -v now '%(%s)T'
                  duration=$((now - ${{ steps.failback-start.outputs.timestamp }}))
                  echo $duration
                  if [ "$duration" -gt "1500" ]; then
                  echo "::error ::Failback of ${{ inputs.helmChartVersion }} is taking longer than 25 minutes"
                  fi

            - name: Get failed Pods info - ${{ env.CLUSTER_0_NAMESPACE }}
              timeout-minutes: 10
              if: failure()
              working-directory: ./test
              run: |
                  set -euo pipefail

                  mkdir -p ./logs

                  kubectl -n "${{ env.CLUSTER_0_NAMESPACE }}" get po
                  kubectl -n "${{ env.CLUSTER_0_NAMESPACE }}" get po | grep -v "Completed" | awk '/0\//{print $1}' | while read -r pod_name; do
                    echo -e "\n###Failed Pod: ${pod_name}###\n";
                    kubectl -n "${{ env.CLUSTER_0_NAMESPACE }}" describe po "$pod_name";
                    kubectl -n "${{ env.CLUSTER_0_NAMESPACE }}" logs "$pod_name" > "./logs/${pod_name}_${{ env.CLUSTER_0_NAMESPACE }}_logs.txt" 2>&1 || true;
                  done

            - name: Get failed Pods info - ${{ env.CLUSTER_1_NAMESPACE }}
              timeout-minutes: 10
              if: failure()
              working-directory: ./test
              run: |
                  set -euo pipefail

                  mkdir -p ./logs

                  kubectl -n "${{ env.CLUSTER_1_NAMESPACE }}" get po
                  kubectl -n "${{ env.CLUSTER_1_NAMESPACE }}" get po | grep -v "Completed" | awk '/0\//{print $1}' | while read -r pod_name; do
                    echo -e "\n###Failed Pod: ${pod_name}###\n";
                    kubectl -n "${{ env.CLUSTER_1_NAMESPACE }}" describe po "$pod_name";
                    kubectl -n "${{ env.CLUSTER_1_NAMESPACE }}" logs "$pod_name" > "./logs/${pod_name}_${{ env.CLUSTER_1_NAMESPACE }}_logs.txt" 2>&1 || true;
                  done

            - name: Delete namespaces
              if: always() && inputs.deployOnly == false && inputs.keepAlive == false
              run: |
                  set -euxo pipefail

                  kubectl delete namespace "${CLUSTER_0_NAMESPACE}" || true
                  kubectl delete namespace "${CLUSTER_1_NAMESPACE}" || true

            - name: Upload migration logs
              if: always() && inputs.migrationEnabled == true
              uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4
              with:
                  name: migration-logs
                  path: |
                      ./test/migration_logs/

            - name: Upload failed logs
              if: failure()
              uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4
              with:
                  name: failure-logs
                  path: |
                      ./test/logs/
