---
name: reusable_teleport_operational_procedure

on:
    workflow_call:
        inputs:
            helmChartVersion:
                type: string
                description: Version of the Helm chart to deploy.
                required: true

            helmChartName:
                description: Optional Helm chart name
                type: string

            globalImageTag:
                description: Optional global image tag
                type: string

            awsProfile:
                description: AWS profile to use
                type: string
                default: infraex

            testsTfBinaryName:
                description: Terraform binary name for tests
                type: string
                default: terraform

            namespacePrefix:
                description: Prefix for namespaces
                type: string
                default: infraex-

            clusterName:
                description: EKS cluster name
                type: string
                default: camunda-ci-eks

            labels:
                description: Labels to apply
                type: string
                default: camunda.cloud/ephemeral=true
            annotations:
                description: 'Annotations to apply (format: key1=val1 key2=val2)'
                required: false
                type: string
                default: janitor/ttl=2h

            cluster:
                description: Cluster identifier (same for Cluster 0 & 1)
                type: string
                default: camunda.teleport.sh-camunda-ci-eks

            backupBucket:
                description: Backup bucket name
                type: string
                default: tests-c8-multi-region-es-eu-central-1

            helmReleaseName:
                description: Helm release name
                type: string
                default: camunda

            zeebeClusterSize:
                description: Zeebe cluster size
                type: number
                default: 8

            deployOnly:
                description: If true, only deploy Camunda and skip failover/failback tests
                type: boolean
                default: false

            migrationEnabled:
                description: If true, enable migration mode
                type: boolean
                default: false
            cluster_0_namespace:
                description: Allows setting a custom namespace for cluster 0
                type: string
            cluster_1_namespace:
                description: Allows setting a custom namespace for cluster 1
                type: string
            backup_name:
                description: Allows setting a custom backup name
                type: string
            overwriteBranch:
                description: Optional branch to checkout instead of inferring from helm chart version
                type: string
            keepAlive:
                description: Whether to keep the clusters alive after the procedure
                type: boolean
                default: false
            skipFailover:
                description: Whether to skip the tests after deployment
                type: boolean
                default: false
            skipFailback:
                description: Whether to skip the tests after deployment
                type: boolean
                default: false
            skipMultiTenancy:
                description: Whether to skip multi-tenancy tests after deployment
                type: boolean
                default: false
        outputs:
            CLUSTER_0_NAMESPACE:
                description: Namespace for cluster 0
                value: ${{ jobs.teleport-setup.outputs.CLUSTER_0_NAMESPACE }}
            CLUSTER_1_NAMESPACE:
                description: Namespace for cluster 1
                value: ${{ jobs.teleport-setup.outputs.CLUSTER_1_NAMESPACE }}
            BACKUP_NAME:
                description: Random ID used for backup name
                value: ${{ jobs.teleport-setup.outputs.BACKUP_NAME }}

        secrets:
            VAULT_ADDR:
                required: true
            VAULT_ROLE_ID:
                required: true
            VAULT_SECRET_ID:
                required: true

permissions:
    contents: read
    id-token: write

env:
    AWS_PROFILE: ${{ inputs.awsProfile }}
    TESTS_TF_BINARY_NAME: ${{ inputs.testsTfBinaryName }}
    NAMESPACE_PREFIX: ${{ inputs.namespacePrefix }}
    CLUSTER_NAME: ${{ inputs.clusterName }}
    LABELS: ${{ inputs.labels }}
    ANNOTATIONS: ${{ inputs.annotations }}

    # Single input used for both cluster variables (to be able to reuse dual cluster tests)
    CLUSTER_0: ${{ inputs.cluster }}
    CLUSTER_1: ${{ inputs.cluster }}

    KUBECONFIG: ./kubeconfig

    BACKUP_BUCKET: ${{ inputs.backupBucket }}

    CAMUNDA_RELEASE_NAME: ${{ inputs.helmReleaseName }}

    ZEEBE_CLUSTER_SIZE: ${{ inputs.zeebeClusterSize }}
    HELM_CHART_VERSION: ${{ inputs.helmChartVersion }}

    CLUSTER_1_NAMESPACE: c8-multiregion-test-cluster-1
    CLUSTER_0_NAMESPACE: c8-multiregion-test-cluster-0

    TELEPORT: true

jobs:
    teleport-setup:
        runs-on: ubuntu-latest
        outputs:
            CLUSTER_0_NAMESPACE: ${{ steps.update-namespaces.outputs.CLUSTER_0_NAMESPACE }}
            CLUSTER_1_NAMESPACE: ${{ steps.update-namespaces.outputs.CLUSTER_1_NAMESPACE }}
            BACKUP_NAME: ${{ steps.update-namespaces.outputs.BACKUP_NAME }}

        steps:
            - name: Set optional environment variables conditionally
              run: |
                  if [ -n "${{ inputs.helmChartName }}" ]; then
                    echo "HELM_CHART_NAME=${{ inputs.helmChartName }}" >> "$GITHUB_ENV"
                  fi
                  if [ -n "${{ inputs.globalImageTag }}" ]; then
                    echo "GLOBAL_IMAGE_TAG=${{ inputs.globalImageTag }}" >> "$GITHUB_ENV"
                  fi

            - name: Map Helm version to branch
              id: helm-version
              run: |
                  version=${{ inputs.helmChartVersion }}

                  major_version=$(echo "$version" | cut -d '.' -f 1)

                  # export major version for test purposes to target specific versions
                  echo "MAJOR_VERSION=$major_version" >> "$GITHUB_ENV"

                  if [[ $version == "SNAPSHOT" ]]; then
                    # we use the snapshot alpha chart of the Helm Chart
                    # We use the snapshot image of camunda/camunda
                    {
                        echo "HELM_CHART_VERSION=0.0.0-snapshot-alpha"
                        echo "HELM_CHART_NAME=oci://ghcr.io/camunda/helm/camunda-platform"
                        echo "MAJOR_VERSION=99"
                    } >> "$GITHUB_ENV"
                    if [[ "$GITHUB_EVENT_NAME" == "pull_request" ]]; then
                      echo "branch=$GITHUB_HEAD_REF" >> "$GITHUB_ENV"
                    else
                      echo "branch=main" >> "$GITHUB_ENV"
                    fi
                  else
                    c8_version=$(curl -X 'GET' -s \
                    "https://artifacthub.io/api/v1/packages/helm/camunda/camunda-platform/${version}" \
                    -H "accept: application/json" | jq -r .app_version)
                    minor_version=$(echo "$c8_version" | cut -d '.' -f 2)
                    echo "branch=stable/8.${minor_version}" >> "$GITHUB_ENV"

                    # For PRs, overwrite to the head ref if the base ref matches the detected minor version
                    if [[ "$GITHUB_EVENT_NAME" == "pull_request" && "$GITHUB_BASE_REF" =~ 8.${minor_version} ]]; then
                      echo "Detected PR against matching stable branch, using head ref"
                      echo "branch=$GITHUB_HEAD_REF" >> "$GITHUB_ENV"
                    fi
                  fi

                  # For workflow_dispatch events, use the ref that triggered the workflow
                  if [[ "$GITHUB_EVENT_NAME" == "workflow_dispatch" ]]; then
                    echo "Detected workflow_dispatch event, using triggered ref - $GITHUB_REF_NAME"
                    echo "branch=$GITHUB_REF_NAME" >> "$GITHUB_ENV"
                  fi

                  if [ -n "${{ inputs.overwriteBranch }}" ]; then
                    echo "Overwriting branch to checkout with input: ${{ inputs.overwriteBranch }}"
                    echo "branch=${{ inputs.overwriteBranch }}" >> "$GITHUB_ENV"
                  fi

            - name: Checkout repository
              uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6
              with:
                  ref: ${{ env.branch }}

            - name: Setup AWS and Tools
              uses: ./.github/actions/setup-aws
              with:
                  secrets: ${{ toJSON(secrets) }}

            - name: Install extra Go and use inbuilt caching
              uses: actions/setup-go@4dc6199c7b1a012772edbd06daecab0f50c9053c # v6
              with:
                  go-version-file: ./test/go.mod
                  cache-dependency-path: ./test/go.sum

            - name: Set up Teleport
              uses: teleport-actions/setup@a820ebbf1bc1a496efca348ad21042d6e8df73a6 # v1
              with:
                  # renovate: datasource=custom.teleport-camunda depName=teleport
                  version: 18.4.1

            - name: Authenticate with Teleport
              uses: teleport-actions/auth-k8s@171cc0ad4b6b7cebcb33c672defe3f6dc58967ba # v2
              with:
                  proxy: camunda.teleport.sh:443
                  token: infra-ci-prod-github-action-infraex
                  kubernetes-cluster: camunda-ci-eks
                  certificate-ttl: 3h # Set a TTL for the certificate matching the job duration (worst case)

            - name: Write kubeconfig file
              id: write-kubeconfig
              run: |
                  kubectl config view --raw > ./test/kubeconfig

            - name: Update namespaces with prefix and random suffix
              id: update-namespaces
              run: |
                  set -euxo pipefail

                  RANDOM_ID="$(openssl rand -hex 3)"

                  BACKUP_NAME=${RANDOM_ID}
                  CLUSTER_1_NAMESPACE="${NAMESPACE_PREFIX}${CLUSTER_1_NAMESPACE}-${RANDOM_ID}"
                  CLUSTER_0_NAMESPACE="${NAMESPACE_PREFIX}${CLUSTER_0_NAMESPACE}-${RANDOM_ID}"

                  # Override with inputs if provided
                  if [ -n "${{ inputs.cluster_0_namespace }}" ]; then
                    CLUSTER_0_NAMESPACE="${{ inputs.cluster_0_namespace }}"
                  fi

                  if [ -n "${{ inputs.cluster_1_namespace }}" ]; then
                    CLUSTER_1_NAMESPACE="${{ inputs.cluster_1_namespace }}"
                  fi

                  if [ -n "${{ inputs.backup_name }}" ]; then
                    RANDOM_ID="${{ inputs.backup_name }}"
                  fi

                  # Write the updated values to the GitHub Actions environment for subsequent steps.
                  {
                    echo "BACKUP_NAME=${BACKUP_NAME}"
                    echo "CLUSTER_1_NAMESPACE=${CLUSTER_1_NAMESPACE}"
                    echo "CLUSTER_0_NAMESPACE=${CLUSTER_0_NAMESPACE}"
                    echo "CAMUNDA_NAMESPACE_0=${CLUSTER_0_NAMESPACE}"
                    echo "CAMUNDA_NAMESPACE_1=${CLUSTER_1_NAMESPACE}"
                  } | tee -a "$GITHUB_ENV" "$GITHUB_OUTPUT"


            - name: Import Secrets
              if: inputs.migrationEnabled == false
              id: secrets
              uses: hashicorp/vault-action@4c06c5ccf5c0761b6029f56cfb1dcf5565918a3b # v3
              with:
                  url: ${{ secrets.VAULT_ADDR }}
                  method: approle
                  roleId: ${{ secrets.VAULT_ROLE_ID }}
                  secretId: ${{ secrets.VAULT_SECRET_ID }}
                  secrets: |
                      secret/data/products/infrastructure-experience/ci/common AWS_ACCESS_KEY | S3_BACKUP_ACCESS_KEY;
                      secret/data/products/infrastructure-experience/ci/common AWS_SECRET_KEY | S3_BACKUP_SECRET_KEY;

            - name: Create namespaces and secrets
              if: inputs.migrationEnabled == false
              id: create-namespaces
              env:
                  AWS_SECRET_ACCESS_KEY_ES: ${{ steps.secrets.outputs.S3_BACKUP_SECRET_KEY }}
                  AWS_ACCESS_KEY_ES: ${{ steps.secrets.outputs.S3_BACKUP_ACCESS_KEY }}
              working-directory: ./test
              run: |
                  set -euxo pipefail
                  go test --count=1 -v -timeout 9m -run TestClusterPrerequisites

            - name: Label and annotate namespaces and secrets
              if: inputs.migrationEnabled == false
              run: |
                  set -euxo pipefail

                  apply_labels_and_annotations() {
                    local namespace="$1"

                    # Apply labels to namespace
                    if [ -n "${LABELS:-}" ]; then
                      kubectl label namespace "$namespace" "${LABELS}" --overwrite
                    fi

                    # Apply annotations to namespace
                    if [ -n "${ANNOTATIONS:-}" ]; then
                      for ann in ${ANNOTATIONS}; do
                        kubectl annotate namespace "$namespace" "$ann" --overwrite
                      done
                    fi

                    # Apply labels and annotations to all secrets in the namespace
                    local secrets
                    secrets="$(kubectl get secrets -n "$namespace" -o jsonpath='{.items[*].metadata.name}')"
                    for secret in $secrets; do
                      if [ -n "${LABELS:-}" ]; then
                        kubectl label secret "$secret" -n "$namespace" "${LABELS}" --overwrite
                      fi
                      if [ -n "${ANNOTATIONS:-}" ]; then
                        for ann in ${ANNOTATIONS}; do
                          kubectl annotate secret "$secret" -n "$namespace" "$ann" --overwrite
                        done
                      fi
                    done
                  }

                  # Combine cluster-0 and cluster-1 arrays into one comma-separated list
                  IFS=',' read -r -a ALL_NAMESPACES <<< "${CLUSTER_0_NAMESPACE},${CLUSTER_1_NAMESPACE}"
                  for namespace in "${ALL_NAMESPACES[@]}"; do
                    apply_labels_and_annotations "$namespace"
                  done

            - name: Debug - Check cluster and node information
              if: inputs.migrationEnabled == false
              run: |
                  set -euxo pipefail
                  echo "=== DEBUG: Cluster Info ==="
                  kubectl cluster-info

                  echo "=== DEBUG: Node List and Status ==="
                  kubectl get nodes -o wide

                  echo "=== DEBUG: Node Labels (for scheduling) ==="
                  kubectl get nodes --show-labels | head -20

                  echo "=== DEBUG: Node Taints ==="
                  kubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints

                  echo "=== DEBUG: Available node groups ==="
                  kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.metadata.labels.nodegroup}{"\n"}{end}'

            - name: Debug - Check NetworkPolicies
              if: inputs.migrationEnabled == false
              run: |
                  set -euxo pipefail
                  echo "=== DEBUG: NetworkPolicies in all namespaces ==="
                  kubectl get networkpolicies -A || echo "No NetworkPolicies found"

                  echo "=== DEBUG: NetworkPolicies in cluster-0 namespace ==="
                  kubectl get networkpolicies -n "${CLUSTER_0_NAMESPACE}" -o yaml || echo "No NetworkPolicies in ${CLUSTER_0_NAMESPACE}"

                  echo "=== DEBUG: NetworkPolicies in cluster-1 namespace ==="
                  kubectl get networkpolicies -n "${CLUSTER_1_NAMESPACE}" -o yaml || echo "No NetworkPolicies in ${CLUSTER_1_NAMESPACE}"

            - name: Debug - Test cross-namespace DNS resolution
              if: inputs.migrationEnabled == false
              run: |
                  set -euxo pipefail
                  echo "=== DEBUG: Creating DNS test pods ==="

                  # Create test pods in each namespace
                  kubectl run dns-test-0 --namespace="${CLUSTER_0_NAMESPACE}" --image=busybox:1.36 --restart=Never --command -- sleep 300 || true
                  kubectl run dns-test-1 --namespace="${CLUSTER_1_NAMESPACE}" --image=busybox:1.36 --restart=Never --command -- sleep 300 || true

                  # Wait for pods to be ready
                  echo "=== DEBUG: Waiting for test pods to be ready ==="
                  kubectl wait --for=condition=ready pod/dns-test-0 -n "${CLUSTER_0_NAMESPACE}" --timeout=120s || echo "WARNING: dns-test-0 not ready"
                  kubectl wait --for=condition=ready pod/dns-test-1 -n "${CLUSTER_1_NAMESPACE}" --timeout=120s || echo "WARNING: dns-test-1 not ready"

                  echo "=== DEBUG: Test pods status ==="
                  kubectl get pod dns-test-0 -n "${CLUSTER_0_NAMESPACE}" -o wide || true
                  kubectl get pod dns-test-1 -n "${CLUSTER_1_NAMESPACE}" -o wide || true

                  echo "=== DEBUG: CoreDNS configuration ==="
                  kubectl get configmap coredns -n kube-system -o yaml || echo "CoreDNS configmap not found"

                  echo "=== DEBUG: Test DNS resolution from cluster-0 to cluster-1 namespace ==="
                  kubectl exec -n "${CLUSTER_0_NAMESPACE}" dns-test-0 -- \
                    nslookup kubernetes.default.svc.cluster.local || echo "Basic DNS failed"
                  kubectl exec -n "${CLUSTER_0_NAMESPACE}" dns-test-0 -- \
                    nslookup "${CLUSTER_1_NAMESPACE}.svc.cluster.local" || \
                    echo "Cross-namespace DNS resolution failed"

                  echo "=== DEBUG: Test DNS resolution from cluster-1 to cluster-0 namespace ==="
                  kubectl exec -n "${CLUSTER_1_NAMESPACE}" dns-test-1 -- \
                    nslookup kubernetes.default.svc.cluster.local || echo "Basic DNS failed"
                  kubectl exec -n "${CLUSTER_1_NAMESPACE}" dns-test-1 -- \
                    nslookup "${CLUSTER_0_NAMESPACE}.svc.cluster.local" || \
                    echo "Cross-namespace DNS resolution failed"

                  echo "=== DEBUG: Cleanup test pods ==="
                  kubectl delete pod dns-test-0 -n "${CLUSTER_0_NAMESPACE}" --ignore-not-found --wait=false
                  kubectl delete pod dns-test-1 -n "${CLUSTER_1_NAMESPACE}" --ignore-not-found --wait=false

            - name: Debug - Check namespace and service configuration
              if: inputs.migrationEnabled == false
              run: |
                  set -euxo pipefail
                  echo "=== DEBUG: Namespaces ==="
                  kubectl get namespaces | grep -E "${CLUSTER_0_NAMESPACE}|${CLUSTER_1_NAMESPACE}" || echo "Namespaces not found"

                  echo "=== DEBUG: Services in cluster-0 namespace ==="
                  kubectl get services -n "${CLUSTER_0_NAMESPACE}" -o wide || echo "No services in ${CLUSTER_0_NAMESPACE}"

                  echo "=== DEBUG: Services in cluster-1 namespace ==="
                  kubectl get services -n "${CLUSTER_1_NAMESPACE}" -o wide || echo "No services in ${CLUSTER_1_NAMESPACE}"

                  echo "=== DEBUG: Endpoints in cluster-0 namespace ==="
                  kubectl get endpoints -n "${CLUSTER_0_NAMESPACE}" || echo "No endpoints in ${CLUSTER_0_NAMESPACE}"

                  echo "=== DEBUG: Endpoints in cluster-1 namespace ==="
                  kubectl get endpoints -n "${CLUSTER_1_NAMESPACE}" || echo "No endpoints in ${CLUSTER_1_NAMESPACE}"

            - name: Deploy dual region Camunda - ${{ inputs.helmChartVersion }}
              if: inputs.migrationEnabled == false
              id: deploy-camunda
              working-directory: ./test
              timeout-minutes: 31
              run: |
                  set -euxo pipefail
                  go test --count=1 -v -timeout 30m -run TestAWSDeployDualRegCamunda

            - name: Deploy dual region Camunda with migration - ${{ inputs.helmChartVersion }}
              if: inputs.migrationEnabled == true
              id: deploy-camunda-migration
              working-directory: ./test
              timeout-minutes: 46
              run: |
                  set -euxo pipefail
                  go test --count=1 -v -timeout 45m -run TestMigrationDualReg

                  # 8.7 process instance created during deploy offset
                  echo "MIGRATION_OFFSET=1" >> "$GITHUB_ENV"

                  mkdir -p ./migration_logs

                  kubectl logs -l app.kubernetes.io/component=orchestration-importer \
                    --tail=-1 \
                    --timestamps=true \
                    --prefix=true \
                    --all-containers=true \
                    -n ${{ env.CLUSTER_0_NAMESPACE }} > "./migration_logs/migration_${{ env.CLUSTER_0_NAMESPACE }}_logs.txt"
                  kubectl logs -l app.kubernetes.io/component=orchestration-importer \
                    --tail=-1 \
                    --timestamps=true \
                    --prefix=true \
                    --all-containers=true \
                    -n ${{ env.CLUSTER_0_NAMESPACE }} > "./migration_logs/importer_${{ env.CLUSTER_0_NAMESPACE }}_logs.txt"

                  kubectl logs -l app.kubernetes.io/component=orchestration-migration \
                    --tail=-1 \
                    --timestamps=true \
                    --prefix=true \
                    --all-containers=true \
                    -n ${{ env.CLUSTER_1_NAMESPACE }} > "./migration_logs/migration_${{ env.CLUSTER_1_NAMESPACE }}_logs.txt"
                  kubectl logs -l app.kubernetes.io/component=orchestration-migration \
                    --tail=-1 \
                    --timestamps=true \
                    --prefix=true \
                    --all-containers=true \
                    -n ${{ env.CLUSTER_1_NAMESPACE }} > "./migration_logs/importer_${{ env.CLUSTER_1_NAMESPACE }}_logs.txt"

            - name: Show pending pod count in each namespace after initial deployment
              id: pending-check-1
              if: always()
              run: |
                  echo "===== Checking pending pods in CLUSTER_0_NAMESPACE after initial deployment ====="
                  # List all pods that are in Pending status, then count them.
                  kubectl get pods -n "${CLUSTER_0_NAMESPACE}" --field-selector=status.phase=Pending
                  num_pending_0=$(kubectl get pods -n "${CLUSTER_0_NAMESPACE}" --field-selector=status.phase=Pending --no-headers | wc -l)
                  echo "Number of pending pods in ${CLUSTER_0_NAMESPACE}: ${num_pending_0}"

                  echo "===== Checking pending pods in CLUSTER_1_NAMESPACE after initial deployment ====="
                  kubectl get pods -n "${CLUSTER_1_NAMESPACE}" --field-selector=status.phase=Pending
                  num_pending_1=$(kubectl get pods -n "${CLUSTER_1_NAMESPACE}" --field-selector=status.phase=Pending --no-headers | wc -l)
                  echo "Number of pending pods in ${CLUSTER_1_NAMESPACE}: ${num_pending_1}"

                  total_pending=$((num_pending_0 + num_pending_1))
                  if [ "$total_pending" -gt 0 ]; then
                   echo "::error ::PendingPodsError: Found $total_pending pending pods across namespaces."
                  fi

            - name: Failover New - ${{ inputs.helmChartVersion }}
              if: inputs.deployOnly == false && inputs.skipFailover == false
              working-directory: ./test
              timeout-minutes: 21
              run: |
                  set -euxo pipefail
                  go test --count=1 -v -timeout 20m -run TestAWSDualRegFailover_8_6_plus

            - name: Set start timestamp for Failback
              if: inputs.deployOnly == false && inputs.skipFailback == false
              id: failback-start
              run: |
                  set -euxo pipefail
                  printf 'timestamp=%(%s)T\n' >> "$GITHUB_OUTPUT"
            - name: Failback New - ${{ inputs.helmChartVersion }}
              if: inputs.deployOnly == false && inputs.skipFailback == false
              working-directory: ./test
              timeout-minutes: 46
              run: |
                  set -euxo pipefail
                  go test --count=1 -v -timeout 45m -run TestAWSDualRegFailback_8_6_plus

            - name: Show pending pod count in each namespace after failback-start
              id: pending-check-2
              if: always() && inputs.deployOnly == false && inputs.skipFailback == false
              run: |
                  echo "===== Checking pending pods in CLUSTER_0_NAMESPACE after Failback ====="
                  # List all pods that are in Pending status, then count them.
                  kubectl get pods -n "${CLUSTER_0_NAMESPACE}" --field-selector=status.phase=Pending
                  num_pending_0=$(kubectl get pods -n "${CLUSTER_0_NAMESPACE}" --field-selector=status.phase=Pending --no-headers | wc -l)
                  echo "Number of pending pods in ${CLUSTER_0_NAMESPACE}: ${num_pending_0}"

                  echo "===== Checking pending pods in CLUSTER_1_NAMESPACE after Failback ====="
                  kubectl get pods -n "${CLUSTER_1_NAMESPACE}" --field-selector=status.phase=Pending
                  num_pending_1=$(kubectl get pods -n "${CLUSTER_1_NAMESPACE}" --field-selector=status.phase=Pending --no-headers | wc -l)
                  echo "Number of pending pods in ${CLUSTER_1_NAMESPACE}: ${num_pending_1}"

                  total_pending=$((num_pending_0 + num_pending_1))
                  if [ "$total_pending" -gt 0 ]; then
                   echo "::error ::PendingPodsError: Found $total_pending pending pods across namespaces."
                  fi

            - name: Calculate Failback duration
              if: inputs.deployOnly == false && inputs.skipFailback == false
              run: |
                  set -euxo pipefail
                  printf -v now '%(%s)T'
                  duration=$((now - ${{ steps.failback-start.outputs.timestamp }}))
                  echo $duration
                  if [ "$duration" -gt "1500" ]; then
                  echo "::error ::Failback of ${{ inputs.helmChartVersion }} is taking longer than 25 minutes"
                  fi

            - name: Test multi-tenancy
              if: inputs.deployOnly == false && inputs.skipMultiTenancy == false && fromJSON(env.MAJOR_VERSION) >= 13
              working-directory: ./test
              timeout-minutes: 21
              run: |
                  set -euxo pipefail
                  go test --count=1 -v -timeout 20m -run TestMultiTenancyDualReg

            - name: Get failed Pods info - ${{ env.CLUSTER_0_NAMESPACE }}
              timeout-minutes: 10
              if: failure()
              working-directory: ./test
              run: |
                  set -euo pipefail

                  mkdir -p ./logs

                  kubectl -n "${{ env.CLUSTER_0_NAMESPACE }}" get po
                  kubectl -n "${{ env.CLUSTER_0_NAMESPACE }}" get po | grep -v "Completed" | awk '/0\//{print $1}' | while read -r pod_name; do
                    echo -e "\n###Failed Pod: ${pod_name}###\n";
                    kubectl -n "${{ env.CLUSTER_0_NAMESPACE }}" describe po "$pod_name";
                    kubectl -n "${{ env.CLUSTER_0_NAMESPACE }}" logs "$pod_name" > "${pod_name}_${{ env.CLUSTER_0_NAMESPACE }}logs.txt";
                  done

            - name: Get failed Pods info - ${{ env.CLUSTER_1_NAMESPACE }}
              timeout-minutes: 10
              if: failure()
              run: |
                  set -euo pipefail

                  mkdir -p ./logs

                  kubectl -n "${{ env.CLUSTER_1_NAMESPACE }}" get po
                  kubectl -n "${{ env.CLUSTER_1_NAMESPACE }}" get po | grep -v "Completed" | awk '/0\//{print $1}' | while read -r pod_name; do
                    echo -e "\n###Failed Pod: ${pod_name}###\n";
                    kubectl -n "${{ env.CLUSTER_1_NAMESPACE }}" describe po "$pod_name";
                    kubectl -n "${{ env.CLUSTER_1_NAMESPACE }}" logs "$pod_name" > "${pod_name}_${{ env.CLUSTER_1_NAMESPACE }}_logs.txt";
                  done

            - name: Debug - Zeebe cluster formation analysis on failure
              timeout-minutes: 10
              if: failure()
              run: |
                  set -euo pipefail

                  echo "=== DEBUG: Zeebe pods detailed status ==="
                  echo "--- Cluster 0 Zeebe pods ---"
                  kubectl get pods -n "${{ env.CLUSTER_0_NAMESPACE }}" -l app.kubernetes.io/component=zeebe-broker -o wide || true

                  echo "--- Cluster 1 Zeebe pods ---"
                  kubectl get pods -n "${{ env.CLUSTER_1_NAMESPACE }}" -l app.kubernetes.io/component=zeebe-broker -o wide || true

                  echo "=== DEBUG: Zeebe services ==="
                  echo "--- Cluster 0 services ---"
                  kubectl get svc -n "${{ env.CLUSTER_0_NAMESPACE }}" | grep -E "zeebe|camunda" || true

                  echo "--- Cluster 1 services ---"
                  kubectl get svc -n "${{ env.CLUSTER_1_NAMESPACE }}" | grep -E "zeebe|camunda" || true

                  echo "=== DEBUG: Zeebe endpoints ==="
                  echo "--- Cluster 0 endpoints ---"
                  kubectl get endpoints -n "${{ env.CLUSTER_0_NAMESPACE }}" | grep -E "zeebe|camunda" || true

                  echo "--- Cluster 1 endpoints ---"
                  kubectl get endpoints -n "${{ env.CLUSTER_1_NAMESPACE }}" | grep -E "zeebe|camunda" || true

                  echo "=== DEBUG: Zeebe StatefulSet details ==="
                  echo "--- Cluster 0 StatefulSet ---"
                  kubectl describe statefulset camunda-zeebe -n "${{ env.CLUSTER_0_NAMESPACE }}" || true

                  echo "--- Cluster 1 StatefulSet ---"
                  kubectl describe statefulset camunda-zeebe -n "${{ env.CLUSTER_1_NAMESPACE }}" || true

            - name: Debug - Cross-namespace connectivity test on failure
              timeout-minutes: 10
              if: failure()
              run: |
                  set -euo pipefail

                  echo "=== DEBUG: Testing cross-namespace connectivity after failure ==="

                  # Get a running pod from each namespace to test connectivity
                  POD_0=$(kubectl get pods -n "${{ env.CLUSTER_0_NAMESPACE }}" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
                  POD_1=$(kubectl get pods -n "${{ env.CLUSTER_1_NAMESPACE }}" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")

                  if [ -n "$POD_0" ]; then
                    echo "--- Testing from pod $POD_0 in cluster-0 ---"
                    echo "DNS resolution test to cluster-1 namespace:"
                    kubectl exec -n "${{ env.CLUSTER_0_NAMESPACE }}" "$POD_0" -- \
                      cat /etc/resolv.conf 2>/dev/null || echo "Cannot read resolv.conf"
                    kubectl exec -n "${{ env.CLUSTER_0_NAMESPACE }}" "$POD_0" -- \
                      nslookup camunda-zeebe.${{ env.CLUSTER_1_NAMESPACE }}.svc.cluster.local \
                      2>/dev/null || echo "DNS lookup failed"

                    echo "Network connectivity test:"
                    kubectl exec -n "${{ env.CLUSTER_0_NAMESPACE }}" "$POD_0" -- \
                      nc -zv -w 5 camunda-zeebe.${{ env.CLUSTER_1_NAMESPACE }}.svc.cluster.local 26502 \
                      2>&1 || echo "Connection test failed"
                  else
                    echo "No pods found in ${{ env.CLUSTER_0_NAMESPACE }}"
                  fi

                  if [ -n "$POD_1" ]; then
                    echo "--- Testing from pod $POD_1 in cluster-1 ---"
                    echo "DNS resolution test to cluster-0 namespace:"
                    kubectl exec -n "${{ env.CLUSTER_1_NAMESPACE }}" "$POD_1" -- \
                      cat /etc/resolv.conf 2>/dev/null || echo "Cannot read resolv.conf"
                    kubectl exec -n "${{ env.CLUSTER_1_NAMESPACE }}" "$POD_1" -- \
                      nslookup camunda-zeebe.${{ env.CLUSTER_0_NAMESPACE }}.svc.cluster.local \
                      2>/dev/null || echo "DNS lookup failed"

                    echo "Network connectivity test:"
                    kubectl exec -n "${{ env.CLUSTER_1_NAMESPACE }}" "$POD_1" -- \
                      nc -zv -w 5 camunda-zeebe.${{ env.CLUSTER_0_NAMESPACE }}.svc.cluster.local 26502 \
                      2>&1 || echo "Connection test failed"
                  else
                    echo "No pods found in ${{ env.CLUSTER_1_NAMESPACE }}"
                  fi

            - name: Debug - Collect Zeebe logs on failure
              timeout-minutes: 10
              if: failure()
              run: |
                  set -euo pipefail

                  mkdir -p ./test/logs/zeebe

                  echo "=== DEBUG: Collecting Zeebe broker logs ==="

                  # Collect logs from all Zeebe pods in cluster-0
                  for i in 0 1 2 3; do
                    echo "--- Collecting logs from camunda-zeebe-$i in cluster-0 ---"
                    kubectl logs "camunda-zeebe-$i" -n "${{ env.CLUSTER_0_NAMESPACE }}" --tail=500 \
                      > "./test/logs/zeebe/zeebe-${i}-cluster0.log" 2>&1 || \
                      echo "Failed to get logs for camunda-zeebe-$i in cluster-0"
                  done

                  # Collect logs from all Zeebe pods in cluster-1
                  for i in 0 1 2 3; do
                    echo "--- Collecting logs from camunda-zeebe-$i in cluster-1 ---"
                    kubectl logs "camunda-zeebe-$i" -n "${{ env.CLUSTER_1_NAMESPACE }}" --tail=500 \
                      > "./test/logs/zeebe/zeebe-${i}-cluster1.log" 2>&1 || \
                      echo "Failed to get logs for camunda-zeebe-$i in cluster-1"
                  done

                  echo "=== DEBUG: Collecting Elasticsearch logs ==="
                  for i in 0 1; do
                    kubectl logs "camunda-elasticsearch-master-$i" \
                      -n "${{ env.CLUSTER_0_NAMESPACE }}" --tail=200 \
                      > "./test/logs/zeebe/es-${i}-cluster0.log" 2>&1 || \
                      echo "Failed to get ES logs in cluster-0"
                    kubectl logs "camunda-elasticsearch-master-$i" \
                      -n "${{ env.CLUSTER_1_NAMESPACE }}" --tail=200 \
                      > "./test/logs/zeebe/es-${i}-cluster1.log" 2>&1 || \
                      echo "Failed to get ES logs in cluster-1"
                  done

                  echo "=== DEBUG: Log files collected ==="
                  ls -la ./test/logs/zeebe/ || true

            - name: Debug - Check events on failure
              timeout-minutes: 5
              if: failure()
              run: |
                  set -euo pipefail

                  echo "=== DEBUG: Kubernetes Events in cluster-0 namespace ==="
                  kubectl get events -n "${{ env.CLUSTER_0_NAMESPACE }}" --sort-by='.lastTimestamp' | tail -50 || true

                  echo "=== DEBUG: Kubernetes Events in cluster-1 namespace ==="
                  kubectl get events -n "${{ env.CLUSTER_1_NAMESPACE }}" --sort-by='.lastTimestamp' | tail -50 || true

                  echo "=== DEBUG: Warning events only ==="
                  kubectl get events -n "${{ env.CLUSTER_0_NAMESPACE }}" --field-selector type=Warning || true
                  kubectl get events -n "${{ env.CLUSTER_1_NAMESPACE }}" --field-selector type=Warning || true

            - name: Delete namespaces
              if: always() && inputs.deployOnly == false && inputs.keepAlive == false
              run: |
                  set -euxo pipefail

                  kubectl delete namespace "${CLUSTER_0_NAMESPACE}" || true
                  kubectl delete namespace "${CLUSTER_1_NAMESPACE}" || true

            - name: Upload migration logs
              if: always() && inputs.migrationEnabled == true
              uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
              with:
                  name: migration-logs
                  path: |
                      ./test/migration_logs/

            - name: Upload failed logs
              if: failure()
              uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5
              with:
                  name: failure-logs
                  path: |
                      ./test/logs/
                      ./test/logs/zeebe/
